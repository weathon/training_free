{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de466347",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wg25r/miniconda/envs/mochi/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "image = Image.open(\"generated.png\")\n",
    "\n",
    "inputs = processor(text=[\"a photo of a camouflaged crab\", \"a photo of an easy-to-see crab\"], images=image, return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b18099ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.pixel_values.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a17b70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6416, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n",
    "print(probs[0][1])\n",
    "probs[0][1].backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abed638a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.4080e-04, -3.0572e-04, -7.5578e-04,  ..., -2.8225e-04,\n",
       "           -1.3286e-04, -6.1910e-06],\n",
       "          [ 2.4908e-04,  8.9160e-05, -2.7698e-04,  ..., -4.5672e-05,\n",
       "           -4.6278e-05,  2.0287e-04],\n",
       "          [ 4.0082e-04, -1.5961e-04,  3.0719e-06,  ...,  4.0979e-05,\n",
       "           -2.2251e-04,  1.7725e-04],\n",
       "          ...,\n",
       "          [-5.6736e-04, -1.7783e-04, -2.2961e-04,  ...,  5.9265e-04,\n",
       "            3.3190e-04,  2.4395e-04],\n",
       "          [-1.0303e-04, -3.4927e-05,  6.8434e-05,  ...,  1.4542e-04,\n",
       "            8.1527e-05, -5.0722e-05],\n",
       "          [-3.8030e-05,  1.4807e-04,  2.1165e-04,  ...,  3.6302e-04,\n",
       "            2.2868e-04,  8.5291e-05]],\n",
       "\n",
       "         [[-1.5988e-04, -2.2908e-04, -7.9835e-04,  ..., -1.4919e-04,\n",
       "           -5.0156e-05,  1.5115e-04],\n",
       "          [ 2.2029e-04,  1.2896e-04, -2.2392e-04,  ..., -3.7935e-04,\n",
       "           -1.0394e-04,  7.9444e-05],\n",
       "          [-3.6271e-04,  7.9844e-05, -1.8428e-04,  ..., -3.4500e-04,\n",
       "           -2.5171e-04, -7.5710e-05],\n",
       "          ...,\n",
       "          [-1.2248e-04,  2.7099e-05, -2.1717e-04,  ...,  1.7522e-04,\n",
       "            2.1663e-04,  3.6981e-04],\n",
       "          [ 3.5414e-04,  3.2843e-04,  2.7429e-04,  ...,  3.5979e-05,\n",
       "            2.6248e-04,  2.7082e-04],\n",
       "          [ 7.4420e-04,  4.2899e-04,  6.0349e-04,  ..., -7.3838e-05,\n",
       "           -5.8767e-05, -1.6716e-04]],\n",
       "\n",
       "         [[-1.4010e-04, -1.9143e-04, -3.5084e-04,  ..., -8.2010e-05,\n",
       "           -1.6329e-04, -1.1203e-04],\n",
       "          [-5.7288e-05,  1.9318e-04,  1.7292e-04,  ...,  1.4207e-04,\n",
       "           -8.7739e-05, -2.8839e-05],\n",
       "          [-4.7621e-05, -1.6490e-04,  7.7943e-05,  ...,  5.0421e-05,\n",
       "            1.9980e-04,  1.5493e-05],\n",
       "          ...,\n",
       "          [ 4.8444e-05, -6.1038e-05,  2.1118e-05,  ..., -1.2472e-05,\n",
       "            1.2785e-04,  5.2697e-04],\n",
       "          [ 2.9960e-04,  1.1153e-04,  3.9876e-05,  ...,  2.0109e-04,\n",
       "           -1.0946e-05,  4.5296e-04],\n",
       "          [ 5.7829e-04,  1.8988e-04,  4.0067e-04,  ...,  1.4214e-04,\n",
       "            1.8719e-04,  5.5836e-04]]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.pixel_values.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mochi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
