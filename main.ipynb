{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e84b448e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wg25r/miniconda/envs/mochi/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "A mixture of bf16 and non-bf16 filenames will be loaded.\n",
      "Loaded bf16 filenames:\n",
      "[transformer/diffusion_pytorch_model.bf16-00001-of-00003.safetensors, transformer/diffusion_pytorch_model.safetensors.index.bf16.json, vae/diffusion_pytorch_model.bf16.safetensors, transformer/diffusion_pytorch_model.bf16-00002-of-00003.safetensors, transformer/diffusion_pytorch_model.bf16-00003-of-00003.safetensors]\n",
      "Loaded non-bf16 filenames:\n",
      "[text_encoder/model.safetensors.index.json, text_encoder/model-00001-of-00002.safetensors, text_encoder/model-00001-of-00004.safetensors, text_encoder/model-00002-of-00002.safetensors, text_encoder/model-00003-of-00004.safetensors, text_encoder/model-00004-of-00004.safetensors, text_encoder/model-00002-of-00004.safetensors\n",
      "If this behavior is not expected, please check your folder structure.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 33.68it/s]it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.03it/s]it/s]\n",
      "Loading pipeline components...: 100%|██████████| 5/5 [00:01<00:00,  2.59it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmochi_pipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MochiPipeline\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdiffusers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m export_to_video\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m pipe = \u001b[43mMochiPipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgenmo/mochi-1-preview\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbf16\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Enable memory savings\u001b[39;00m\n\u001b[32m      8\u001b[39m pipe.enable_vae_tiling()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mochi/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:503\u001b[39m, in \u001b[36mDiffusionPipeline.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    501\u001b[39m     module.to(device=device)\n\u001b[32m    502\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_loaded_in_4bit_bnb \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_loaded_in_8bit_bnb \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_group_offloaded:\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m     \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    506\u001b[39m     module.dtype == torch.float16\n\u001b[32m    507\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(device) \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    508\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m silence_dtype_warnings\n\u001b[32m    509\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_offloaded\n\u001b[32m    510\u001b[39m ):\n\u001b[32m    511\u001b[39m     logger.warning(\n\u001b[32m    512\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    513\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not recommended to move them to `cpu` as running them will fail. Please make\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    516\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m `torch_dtype=torch.float16` argument, or use another device for inference.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    517\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mochi/lib/python3.11/site-packages/transformers/modeling_utils.py:3698\u001b[39m, in \u001b[36mPreTrainedModel.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3693\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[32m   3694\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3695\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3696\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3697\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m3698\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mochi/lib/python3.11/site-packages/torch/nn/modules/module.py:1355\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1352\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1353\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mochi/lib/python3.11/site-packages/torch/nn/modules/module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mochi/lib/python3.11/site-packages/torch/nn/modules/module.py:942\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    939\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    940\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    943\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    945\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mochi/lib/python3.11/site-packages/torch/nn/modules/module.py:1341\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1334\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1335\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1336\u001b[39m             device,\n\u001b[32m   1337\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1338\u001b[39m             non_blocking,\n\u001b[32m   1339\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1340\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1342\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1344\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mochi/lib/python3.11/site-packages/torch/cuda/__init__.py:383\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m queued_call, orig_traceback \u001b[38;5;129;01min\u001b[39;00m _queued_calls:\n\u001b[32m    382\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m         \u001b[43mqueued_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    385\u001b[39m         msg = (\n\u001b[32m    386\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCUDA call failed lazily at initialization with error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    387\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCUDA call was originally invoked at:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m.join(orig_traceback)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    388\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mochi/lib/python3.11/site-packages/torch/cuda/__init__.py:251\u001b[39m, in \u001b[36m_check_capability\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.version.cuda \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# on ROCm we don't want this check\u001b[39;00m\n\u001b[32m    250\u001b[39m     CUDA_VERSION = torch._C._cuda_getCompiledVersion()  \u001b[38;5;66;03m# noqa: F841\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mdevice_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[32m    252\u001b[39m         capability = get_device_capability(d)\n\u001b[32m    253\u001b[39m         major = capability[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mochi/lib/python3.11/site-packages/torch/cuda/__init__.py:990\u001b[39m, in \u001b[36mdevice_count\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    988\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _cached_device_count\n\u001b[32m    989\u001b[39m \u001b[38;5;66;03m# bypass _device_count_nvml() if rocm (not supported)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m990\u001b[39m nvml_count = _device_count_amdsmi() \u001b[38;5;28;01mif\u001b[39;00m torch.version.hip \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_device_count_nvml\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    991\u001b[39m r = torch._C._cuda_getDeviceCount() \u001b[38;5;28;01mif\u001b[39;00m nvml_count < \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m nvml_count\n\u001b[32m    992\u001b[39m \u001b[38;5;66;03m# NB: Do not cache the device count prior to CUDA initialization, because\u001b[39;00m\n\u001b[32m    993\u001b[39m \u001b[38;5;66;03m# the number of devices can change due to changes to CUDA_VISIBLE_DEVICES\u001b[39;00m\n\u001b[32m    994\u001b[39m \u001b[38;5;66;03m# setting prior to CUDA initialization.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mochi/lib/python3.11/site-packages/torch/cuda/__init__.py:946\u001b[39m, in \u001b[36m_device_count_nvml\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    942\u001b[39m     visible_devices = _transform_uuid_to_ordinals(\n\u001b[32m    943\u001b[39m         cast(\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m], visible_devices), uuids\n\u001b[32m    944\u001b[39m     )\n\u001b[32m    945\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m946\u001b[39m     raw_cnt = \u001b[43m_raw_device_count_nvml\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raw_cnt <= \u001b[32m0\u001b[39m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m raw_cnt\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mochi/lib/python3.11/site-packages/torch/cuda/__init__.py:787\u001b[39m, in \u001b[36m_raw_device_count_nvml\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    784\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mctypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m byref, c_int, CDLL\n\u001b[32m    786\u001b[39m nvml_h = CDLL(\u001b[33m\"\u001b[39m\u001b[33mlibnvidia-ml.so.1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m rc = \u001b[43mnvml_h\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnvmlInit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m rc != \u001b[32m0\u001b[39m:\n\u001b[32m    789\u001b[39m     warnings.warn(\u001b[33m\"\u001b[39m\u001b[33mCan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt initialize NVML\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from mochi_pipeline import MochiPipeline\n",
    "from diffusers.utils import export_to_video\n",
    "\n",
    "pipe = MochiPipeline.from_pretrained(\"genmo/mochi-1-preview\", variant=\"bf16\", torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "\n",
    "# Enable memory savings\n",
    "pipe.enable_vae_tiling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210047f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    }
   ],
   "source": [
    "from processor import MochiAttnProcessor2_0\n",
    "prompt = \"An apple camflagued in a field of flowers, the color of the apple is red, blended in to the flower makes it hard to see\"\n",
    "from transformers import T5TokenizerFast\n",
    "tokenizer = T5TokenizerFast.from_pretrained(\"genmo/mochi-1-preview\", subfolder=\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f618e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = tokenizer.tokenize(prompt).index(\"▁apple\")\n",
    "index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a77f121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:37<00:00,  3.75s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'mochi.mp4'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for block in pipe.transformer.transformer_blocks:\n",
    "    block.attn1.processor = MochiAttnProcessor2_0(token_index_of_interest=torch.tensor([index])) \n",
    "frames = pipe(prompt,\n",
    "              negative_prompt=\"bad quality, ugly faces, moving camera, easy to swe, stand out, able to see\",\n",
    "              num_inference_steps=10,\n",
    "              guidance_scale=9,\n",
    "              num_frames=30).frames[0]\n",
    "\n",
    "export_to_video(frames, \"mochi.mp4\", fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935155ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1590"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames[0].size[0]//16 * frames[0].size[1]//16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd853f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab\n",
    "import numpy as np\n",
    "\n",
    "extracted_maps = []\n",
    "maps = pipe.attention_maps  \n",
    "for step in range(len(maps)):\n",
    "    for layer in range(len(maps[step])):\n",
    "        # print(maps[step][layer].shape) [B, H, L, D]\n",
    "        map = maps[step][layer][0].mean(0)[0]\n",
    "        extracted_maps.append(map.cpu().float().numpy().reshape(-1, frames[0].size[1]//16, frames[0].size[0]//16))\n",
    "extracted_maps = np.array(extracted_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f382d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 5, 30, 53)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_maps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68ca23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (extracted_maps > extracted_maps.mean(axis=0) + extracted_maps.std(axis=0)).astype(np.float32) + (extracted_maps < extracted_maps.mean(axis=0) - extracted_maps.std(axis=0)).astype(np.float32)\n",
    "extracted_maps[mask != 0] = np.nan\n",
    "mean_map = np.nanmean(np.abs(extracted_maps), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c3114c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 30, 53)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911a6691",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'mochi_attention.mp4'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "def opening(x, kernel_size=3):\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))\n",
    "    x = [cv2.morphologyEx(i, cv2.MORPH_OPEN, kernel) for i in x]\n",
    "    return x\n",
    "\n",
    "def blur(x, kernel_size=3):\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.float32) / (kernel_size * kernel_size)\n",
    "    x = [cv2.filter2D(i, -1, kernel) for i in x]\n",
    "    return x\n",
    "\n",
    "def resize(x, size):\n",
    "    return [cv2.resize(i, size) for i in x]\n",
    "    \n",
    "    \n",
    "def normalize(x):\n",
    "    return [cv2.normalize(i, None, 0, 1, cv2.NORM_MINMAX) for i in x]\n",
    "\n",
    "def compose_frames(frames, maps):\n",
    "    frames = [np.array(frame).astype(float)/255 for frame in frames]\n",
    "    for i in range(len(frames)):\n",
    "        map = cv2.cvtColor(maps[i//6], cv2.COLOR_GRAY2RGB).astype(float)       \n",
    "        frames[i] = cv2.addWeighted(frames[i], 0.5, map, 0.5, 0)\n",
    "    return frames\n",
    "    \n",
    "    \n",
    "mean_map = opening(mean_map, kernel_size=3)\n",
    "mean_map = blur(mean_map, kernel_size=3)\n",
    "mean_map = resize(mean_map, size=(frames[0].size[0], frames[0].size[1]))\n",
    "mean_map = normalize(mean_map)\n",
    "video = compose_frames(frames, mean_map)\n",
    "export_to_video(video, \"mochi_attention.mp4\", fps=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mochi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
