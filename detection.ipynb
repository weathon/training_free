{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12ca27dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wg25r/miniconda/envs/mochi/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "from diffusers import AutoencoderKLMochi\n",
    "from mochi_pipeline import MochiPipeline, linear_quadratic_schedule, retrieve_timesteps\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "from mochi_processor import MochiAttnProcessor2_0\n",
    "import torch\n",
    "import torchvision\n",
    "from pathlib import Path\n",
    "\n",
    "def encode_videos(model: torch.nn.Module, vid_path: Path):\n",
    "    video, _, metadata = torchvision.io.read_video(str(vid_path), output_format=\"THWC\", pts_unit=\"secs\")\n",
    "    video = video.permute(3, 0, 1, 2)\n",
    "    og_shape = video.shape\n",
    "    print(f\"Original video shape: {og_shape}\")\n",
    "    video = video.unsqueeze(0)\n",
    "    video = video.float() / 127.5 - 1.0\n",
    "    video = video.to(model.device)\n",
    "\n",
    "    assert video.ndim == 5\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            ldist = model._encode(video)\n",
    "\n",
    "    return ldist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16a29545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  6.91it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:02<00:00,  1.74it/s]it/s]\n",
      "Loading pipeline components...: 100%|██████████| 5/5 [00:03<00:00,  1.27it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MochiPipeline {\n",
       "  \"_class_name\": \"MochiPipeline\",\n",
       "  \"_diffusers_version\": \"0.34.0.dev0\",\n",
       "  \"_name_or_path\": \"genmo/mochi-1-preview\",\n",
       "  \"force_zeros_for_empty_prompt\": false,\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"FlowMatchEulerDiscreteScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"T5EncoderModel\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"T5Tokenizer\"\n",
       "  ],\n",
       "  \"transformer\": [\n",
       "    \"diffusers\",\n",
       "    \"MochiTransformer3DModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKLMochi\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = \"genmo/mochi-1-preview\"\n",
    "pipeline = MochiPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "\n",
    "pipeline.enable_vae_slicing()\n",
    "pipeline.enable_vae_tiling()\n",
    "pipeline.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17e357c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wg25r/miniconda/envs/mochi/lib/python3.11/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original video shape: torch.Size([3, 0, 1, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4D or 5D (batch mode) tensor with possibly 0 batch size and other non-zero dimensions for input, but got: [1, 64, 0, 1, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m video = \u001b[33m\"\u001b[39m\u001b[33m/home/wg25r/make_it_move/training_free/finetrainers/training/mochi-1/dataset/moth.mp4\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m lattent = \u001b[43mencode_videos\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvae\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mencode_videos\u001b[39m\u001b[34m(model, vid_path)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.inference_mode():\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autocast(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m, dtype=torch.bfloat16):\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m         ldist = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ldist\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mochi/lib/python3.11/site-packages/diffusers/models/autoencoders/autoencoder_kl_mochi.py:877\u001b[39m, in \u001b[36mAutoencoderKLMochi._encode\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    872\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    873\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFrame-wise encoding does not work with the Mochi VAE Encoder due to the presence of attention layers. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    874\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAs intermediate frames are not independent from each other, they cannot be encoded frame-wise.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    875\u001b[39m     )\n\u001b[32m    876\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m877\u001b[39m     enc, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m enc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mochi/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mochi/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mochi/lib/python3.11/site-packages/diffusers/models/autoencoders/autoencoder_kl_mochi.py:527\u001b[39m, in \u001b[36mMochiEncoder3D.forward\u001b[39m\u001b[34m(self, hidden_states, conv_cache)\u001b[39m\n\u001b[32m    523\u001b[39m         hidden_states, new_conv_cache[conv_cache_key] = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    524\u001b[39m             down_block, hidden_states, conv_cache.get(conv_cache_key)\n\u001b[32m    525\u001b[39m         )\n\u001b[32m    526\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m     hidden_states, new_conv_cache[\u001b[33m\"\u001b[39m\u001b[33mblock_in\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblock_in\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconv_cache\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mblock_in\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    531\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, down_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.down_blocks):\n\u001b[32m    532\u001b[39m         conv_cache_key = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdown_block_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mochi/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mochi/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mochi/lib/python3.11/site-packages/diffusers/models/autoencoders/autoencoder_kl_mochi.py:312\u001b[39m, in \u001b[36mMochiMidBlock3D.forward\u001b[39m\u001b[34m(self, hidden_states, conv_cache)\u001b[39m\n\u001b[32m    308\u001b[39m     hidden_states, new_conv_cache[conv_cache_key] = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    309\u001b[39m         resnet, hidden_states, conv_cache.get(conv_cache_key)\n\u001b[32m    310\u001b[39m     )\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     hidden_states, new_conv_cache[conv_cache_key] = \u001b[43mresnet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconv_cache\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconv_cache_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    317\u001b[39m     residual = hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mochi/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mochi/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mochi/lib/python3.11/site-packages/diffusers/models/autoencoders/autoencoder_kl_mochi.py:118\u001b[39m, in \u001b[36mMochiResnetBlock3D.forward\u001b[39m\u001b[34m(self, inputs, conv_cache)\u001b[39m\n\u001b[32m    116\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm1(hidden_states)\n\u001b[32m    117\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.nonlinearity(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m hidden_states, new_conv_cache[\u001b[33m\"\u001b[39m\u001b[33mconv1\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconv_cache\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconv1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm2(hidden_states)\n\u001b[32m    121\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.nonlinearity(hidden_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mochi/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mochi/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mochi/lib/python3.11/site-packages/diffusers/models/autoencoders/autoencoder_kl_cogvideox.py:138\u001b[39m, in \u001b[36mCogVideoXCausalConv3d.forward\u001b[39m\u001b[34m(self, inputs, conv_cache)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: torch.Tensor, conv_cache: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfake_context_parallel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pad_mode == \u001b[33m\"\u001b[39m\u001b[33mreplicate\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    141\u001b[39m         conv_cache = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mochi/lib/python3.11/site-packages/diffusers/models/autoencoders/autoencoder_kl_cogvideox.py:129\u001b[39m, in \u001b[36mCogVideoXCausalConv3d.fake_context_parallel_forward\u001b[39m\u001b[34m(self, inputs, conv_cache)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfake_context_parallel_forward\u001b[39m(\n\u001b[32m    126\u001b[39m     \u001b[38;5;28mself\u001b[39m, inputs: torch.Tensor, conv_cache: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    127\u001b[39m ) -> torch.Tensor:\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pad_mode == \u001b[33m\"\u001b[39m\u001b[33mreplicate\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m         inputs = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtime_causal_padding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreplicate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    131\u001b[39m         kernel_size = \u001b[38;5;28mself\u001b[39m.time_kernel_size\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/mochi/lib/python3.11/site-packages/torch/nn/functional.py:5209\u001b[39m, in \u001b[36mpad\u001b[39m\u001b[34m(input, pad, mode, value)\u001b[39m\n\u001b[32m   5202\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mreplicate\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   5203\u001b[39m             \u001b[38;5;66;03m# Use slow decomp whose backward will be in terms of index_put.\u001b[39;00m\n\u001b[32m   5204\u001b[39m             \u001b[38;5;66;03m# importlib is required because the import cannot be top level\u001b[39;00m\n\u001b[32m   5205\u001b[39m             \u001b[38;5;66;03m# (cycle) and cannot be nested (TS doesn't support)\u001b[39;00m\n\u001b[32m   5206\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\n\u001b[32m   5207\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtorch._decomp.decompositions\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5208\u001b[39m             )._replication_pad(\u001b[38;5;28minput\u001b[39m, pad)\n\u001b[32m-> \u001b[39m\u001b[32m5209\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected 4D or 5D (batch mode) tensor with possibly 0 batch size and other non-zero dimensions for input, but got: [1, 64, 0, 1, 1]"
     ]
    }
   ],
   "source": [
    "video = \"/home/wg25r/make_it_move/training_free/finetrainers/training/mochi-1/dataset/moth.mp4\"\n",
    "lattent = encode_videos(\n",
    "    pipeline.vae,\n",
    "    Path(video),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe4a2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn_like(lattent) * 0.1\n",
    "noisy_latent = lattent * 0.5 + noise * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aadfff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 'a', '▁cam', 'f', 'la', 'gue', 'd', '▁*', 'anim', 'al', '*', '▁hiding', '▁in', '▁the', '▁+', 'back', 'ground', '+']\n",
      "tensor([ 8,  9, 15, 16])\n",
      "tensor([1, 1, 0, 0])\n",
      "tensor([0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "tokenizer = pipeline.tokenizer\n",
    "prompt = \"a camflagued *animal* hiding in the +background+\"\n",
    "positive_start_index = tokenizer.tokenize(prompt).index(\"▁*\")\n",
    "positive_end_index = tokenizer.tokenize(prompt).index(\"*\")\n",
    "\n",
    "negative_start_index = tokenizer.tokenize(prompt).index(\"▁+\")\n",
    "negative_end_index = tokenizer.tokenize(prompt).index(\"+\")\n",
    "\n",
    "print(tokenizer.tokenize(prompt))\n",
    "\n",
    "\n",
    "indices = torch.tensor(list(range(positive_start_index + 1, positive_end_index)) + list(range(negative_start_index + 1, negative_end_index)))# + 1 # plus one because of the <extra_id_0> token\n",
    "print(indices)\n",
    "positive_mask = torch.tensor([1] * (positive_end_index - positive_start_index - 1) + [0] * (negative_end_index - negative_start_index - 1))\n",
    "negative_mask = torch.tensor([0] * (positive_end_index - positive_start_index - 1) + [1] * (negative_end_index - negative_start_index - 1))\n",
    "print(positive_mask)\n",
    "print(negative_mask)\n",
    "\n",
    "threshold_noise = 0.025\n",
    "sigmas = linear_quadratic_schedule(20, threshold_noise)\n",
    "sigmas = np.array(sigmas)\n",
    "timesteps = retrieve_timesteps(\n",
    "            pipeline.scheduler,\n",
    "            num_inference_steps=20,\n",
    "            device=\"cuda\",\n",
    "            timesteps=None,\n",
    "            sigmas=sigmas,\n",
    "        )\n",
    "\n",
    "for block in pipeline.transformer.transformer_blocks:\n",
    "    block.attn1.processor = MochiAttnProcessor2_0(token_index_of_interest=indices, positive_mask=positive_mask) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d6b3f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  0.0000,   2.5000,   5.0000,   7.5000,  10.0000,  12.5000,  15.0000,\n",
       "          17.5000,  20.0000,  22.5000,  25.0000,  37.0000,  68.0000, 118.0000,\n",
       "         187.0000, 275.0000, 382.0000, 508.0000, 653.0000, 817.0001],\n",
       "        device='cuda:0'),\n",
       " 20)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1714d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:39<00:00,  5.64s/it]\n"
     ]
    }
   ],
   "source": [
    "from diffusers.utils import export_to_video\n",
    "\n",
    "video = pipeline(latents=noisy_latent.cuda()[:,:12],\n",
    "         timesteps=timesteps[0][-7:].cpu(),\n",
    "         prompt=prompt, \n",
    "         emphasize_indices=(0,0),\n",
    "         emphasize_neg_indices=(0,0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6bd2fa6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'export_to_video' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mexport_to_video\u001b[49m(np.array(video[\u001b[33m\"\u001b[39m\u001b[33mframes\u001b[39m\u001b[33m\"\u001b[39m])[\u001b[32m0\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33moutput.mp4\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'export_to_video' is not defined"
     ]
    }
   ],
   "source": [
    "export_to_video(np.array(video[\"frames\"])[0], \"output.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d802a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wg25r/miniconda/envs/mochi/lib/python3.11/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n",
      "/home/wg25r/miniconda/envs/mochi/lib/python3.11/site-packages/torchvision/io/video.py:199: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = torchvision.io.read_video(str(\"/home/wg25r/make_it_move/training_free/finetrainers/training/mochi-1/dataset/crab_000.mp4\"), output_format=\"THWC\", pts_unit=\"secs\")[0]\n",
    "extracted_positive_maps = [] \n",
    "extracted_negative_maps = []\n",
    "maps = pipeline.attention_maps  \n",
    "for step in range(max(len(maps) - 10, 0), len(maps)-1):\n",
    "    for layer in range(len(maps[step])): \n",
    "        # print(maps[step][layer].shape)# [B, H, K, Q]\n",
    "        map = maps[step][layer][0].mean(0)[0] # use sum, because in it it was softmax and spread outed\n",
    "        # print(maps[step][layer][0].mean(0)[positive_mask==1].shape) # need to use bool!\n",
    "        extracted_positive_maps.append(map.cpu().float().numpy().reshape(-1, frames[0].shape[0]//16, frames[0].shape[1]//16))\n",
    "        \n",
    "        map = maps[step][layer][0].mean(0)[1]\n",
    "        extracted_negative_maps.append(map.cpu().float().numpy().reshape(-1, frames[0].shape[0]//16, frames[0].shape[1]//16))\n",
    "\n",
    "        # assert torch.all(maps[step][layer][0].mean(0).sum(0) >= 0.99), maps[step][layer][0].mean(0).sum(0)\n",
    "extracted_positive_maps = np.array(extracted_positive_maps)\n",
    "extracted_negative_maps = np.array(extracted_negative_maps)\n",
    "np.isnan(extracted_positive_maps).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c136992e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37, 480, 848)\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4065889/758690003.py:33: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  frames = [np.array(frame).astype(float)/255 for frame in frames]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'map.mp4'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "file_id = 1\n",
    "# %%\n",
    "mask = (extracted_positive_maps > extracted_positive_maps.mean(axis=0) + extracted_positive_maps.std(axis=0)).astype(np.float32) + (extracted_positive_maps < extracted_positive_maps.mean(axis=0) - extracted_positive_maps.std(axis=0)).astype(np.float32)\n",
    "mean_pos_map = np.nanmean(extracted_positive_maps, axis=0)\n",
    "\n",
    "\n",
    "mask = (extracted_negative_maps > extracted_negative_maps.mean(axis=0) + extracted_negative_maps.std(axis=0)).astype(np.float32) + (extracted_negative_maps < extracted_negative_maps.mean(axis=0) - extracted_negative_maps.std(axis=0)).astype(np.float32)\n",
    "mean_neg_map = np.nanmean(extracted_negative_maps, axis=0)\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "def opening(x, kernel_size=3):\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))\n",
    "    x = [cv2.morphologyEx(i, cv2.MORPH_OPEN, kernel) for i in x]\n",
    "    return x\n",
    "\n",
    "def blur(x, kernel_size=3):\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.float32) / (kernel_size * kernel_size)\n",
    "    x = [cv2.filter2D(i, -1, kernel) for i in x]\n",
    "    return x\n",
    "\n",
    "def resize(x, size):\n",
    "    return [cv2.resize(i, size) for i in x]\n",
    "    \n",
    "    \n",
    "def normalize(x):\n",
    "    return [(i - i.min()) / (i.max() - i.min()) for i in x]\n",
    "\n",
    "def compose_frames(frames, maps):\n",
    "    frames = [np.array(frame).astype(float)/255 for frame in frames]\n",
    "    for i in range(len(frames)):\n",
    "        map = cv2.cvtColor(maps[(i-2)//6], cv2.COLOR_GRAY2RGB).astype(float)       \n",
    "        frames[i] = cv2.addWeighted(frames[i], 0.5, map, 0.5, 0)\n",
    "    return frames\n",
    "\n",
    "def repeat_maps(maps, total_len):\n",
    "    export_maps = []\n",
    "    for i in range(total_len):\n",
    "        export_maps.append(maps[math.ceil((i-2)/6)])\n",
    "    return export_maps\n",
    "\n",
    "# print(\"a\")\n",
    "mean_pos_map = opening(mean_pos_map, kernel_size=3)\n",
    "mean_neg_map = opening(mean_neg_map, kernel_size=3)\n",
    "mean_pos_map = blur(mean_pos_map, kernel_size=3)\n",
    "mean_neg_map = blur(mean_neg_map, kernel_size=3)\n",
    "mean_pos_map = resize(mean_pos_map, size=(frames[0].shape[1], frames[0].shape[0]))\n",
    "mean_neg_map = resize(mean_neg_map, size=(frames[0].shape[1], frames[0].shape[0]))\n",
    "\n",
    "mean_pos_map = normalize(mean_pos_map)\n",
    "mean_neg_map = normalize(mean_neg_map)\n",
    "\n",
    "video = repeat_maps(mean_pos_map, len(frames))\n",
    "\n",
    "# video = compose_frames(frames, mean_pos_map)\n",
    "print(np.array(video).shape)\n",
    "export_to_video(video, f\"res/mochi_pos_{file_id:02d}_map.mp4\", fps=30)\n",
    "\n",
    "video = repeat_maps(mean_neg_map, len(frames))\n",
    "# video = compose_frames(frames, mean_neg_map)\n",
    "export_to_video(video, f\"res/mochi_neg_{file_id:02d}_map.mp4\", fps=30)\n",
    "\n",
    "# fg = np.clip(np.array(mean_pos_map) - np.array(mean_neg_map), 0, 100)\n",
    "fg = np.array(mean_pos_map) - np.array(mean_neg_map) * 2\n",
    "fg[fg < 0] = 0\n",
    "print(fg.max())\n",
    "\n",
    "fg = normalize(fg)\n",
    "video = compose_frames(frames, fg) \n",
    "export_to_video(video, f\"map.mp4\", fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7853927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([480, 848, 3]), (480, 848))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames[0].shape, fg[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mochi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
