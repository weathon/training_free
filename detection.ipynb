{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ca27dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from diffusers import AutoencoderKLMochi\n",
    "from mochi_pipeline import MochiPipeline, linear_quadratic_schedule, retrieve_timesteps\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "from mochi_processor import MochiAttnProcessor2_0\n",
    "import torch\n",
    "import torchvision\n",
    "from pathlib import Path\n",
    "from diffusers.utils import export_to_video\n",
    "\n",
    "\n",
    "def encode_videos(model: torch.nn.Module, vid_path: Path):\n",
    "    video, _, metadata = torchvision.io.read_video(str(vid_path), output_format=\"THWC\", pts_unit=\"secs\")\n",
    "    video = video.permute(3, 0, 1, 2)\n",
    "    og_shape = video.shape\n",
    "    \n",
    "    print(f\"Original video shape: {og_shape}\")\n",
    "    video = video.unsqueeze(0)\n",
    "    video = video.float() / 127.5 - 1.0\n",
    "    video = video.to(model.device)\n",
    "\n",
    "    assert video.ndim == 5\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            ldist = model._encode(video)\n",
    "    return ldist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b0ca5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a29545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d79eb6ae283454cae606f412bc23cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b244b9f9924dbd807971bd1d9e77eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "774712fa07fb455d8d224faeaec3a13b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 94.50 GiB of which 54.06 MiB is free. Including non-PyTorch memory, this process has 91.75 GiB memory in use. Of the allocated memory 90.23 GiB is allocated by PyTorch, and 832.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m pipeline\u001b[38;5;241m.\u001b[39menable_vae_slicing()\n\u001b[1;32m      5\u001b[0m pipeline\u001b[38;5;241m.\u001b[39menable_vae_tiling()\n\u001b[0;32m----> 6\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mtokenizer\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/diffusers/pipelines/pipeline_utils.py:482\u001b[0m, in \u001b[0;36mDiffusionPipeline.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m     module\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_loaded_in_4bit_bnb \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_loaded_in_8bit_bnb \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_group_offloaded:\n\u001b[0;32m--> 482\u001b[0m     module\u001b[38;5;241m.\u001b[39mto(device, dtype)\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    485\u001b[0m     module\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(device) \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m silence_dtype_warnings\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_offloaded\n\u001b[1;32m    489\u001b[0m ):\n\u001b[1;32m    490\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    491\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    492\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not recommended to move them to `cpu` as running them will fail. Please make\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `torch_dtype=torch.float16` argument, or use another device for inference.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    496\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/diffusers/models/modeling_utils.py:1353\u001b[0m, in \u001b[0;36mModelMixin.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   1349\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe module \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is group offloaded and moving it using `.to()` is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1350\u001b[0m     )\n\u001b[1;32m   1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m-> 1353\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1355\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 915 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:942\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 942\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[1;32m    943\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1341\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1336\u001b[0m             device,\n\u001b[1;32m   1337\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1338\u001b[0m             non_blocking,\n\u001b[1;32m   1339\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1340\u001b[0m         )\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1342\u001b[0m         device,\n\u001b[1;32m   1343\u001b[0m         dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1344\u001b[0m         non_blocking,\n\u001b[1;32m   1345\u001b[0m     )\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 94.50 GiB of which 54.06 MiB is free. Including non-PyTorch memory, this process has 91.75 GiB memory in use. Of the allocated memory 90.23 GiB is allocated by PyTorch, and 832.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "model_id = \"genmo/mochi-1-preview\"\n",
    "pipeline = MochiPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "\n",
    "pipeline.enable_vae_slicing()\n",
    "pipeline.enable_vae_tiling()\n",
    "pipeline.to(\"cuda\")\n",
    "# import numpy as np\n",
    "# tokenizer = pipeline.tokenizer\n",
    "# prompt = \"a camflagued *animal* moving in the +background+\"\n",
    "# positive_start_index = tokenizer.tokenize(prompt).index(\"▁*\")\n",
    "# positive_end_index = tokenizer.tokenize(prompt).index(\"*\")\n",
    "\n",
    "# negative_start_index = tokenizer.tokenize(prompt).index(\"▁+\")\n",
    "# negative_end_index = tokenizer.tokenize(prompt).index(\"+\")\n",
    "\n",
    "# print(tokenizer.tokenize(prompt))\n",
    "\n",
    "\n",
    "# indices = torch.tensor(list(range(positive_start_index + 1, positive_end_index)) + list(range(negative_start_index + 1, negative_end_index)))# + 1 # plus one because of the <extra_id_0> token\n",
    "# print(indices)\n",
    "# positive_mask = torch.tensor([1] * (positive_end_index - positive_start_index - 1) + [0] * (negative_end_index - negative_start_index - 1))\n",
    "# negative_mask = torch.tensor([0] * (positive_end_index - positive_start_index - 1) + [1] * (negative_end_index - negative_start_index - 1))\n",
    "# print(positive_mask)\n",
    "# print(negative_mask)\n",
    "\n",
    "# threshold_noise = 0.025\n",
    "# sigmas = linear_quadratic_schedule(20, threshold_noise)\n",
    "# sigmas = np.array(sigmas)\n",
    "# timesteps = retrieve_timesteps(\n",
    "#             pipeline.scheduler,\n",
    "#             num_inference_steps=20,\n",
    "#             device=\"cuda\",\n",
    "#             timesteps=None,\n",
    "#             sigmas=sigmas,\n",
    "#         )\n",
    "\n",
    "# for block in pipeline.transformer.transformer_blocks:\n",
    "#     block.attn1.processor = MochiAttnProcessor2_0(token_index_of_interest=indices, positive_mask=positive_mask) \n",
    "\n",
    "# video = pipeline(#latents=noise.cuda(), \n",
    "#         #  timesteps=timesteps[0].cpu(),#[:15].cpu(),\n",
    "#          num_inference_steps=10,\n",
    "#          prompt=\"a cat\", \n",
    "#          negative_prompt=\"this is a nagative prompt\",\n",
    "#          emphasize_indices=(0,0), \n",
    "#          num_frames=12,\n",
    "#          emphasize_neg_indices=(0,0),\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17e357c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/torchvision/io/video.py:199: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original video shape: torch.Size([3, 81, 480, 848])\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    video = \"/home/ubuntu/wash/cog_dataset/crab_1_001.mp4\"\n",
    "    latents = encode_videos(\n",
    "        pipeline.vae,\n",
    "        Path(video),\n",
    "    )[:,:12].to(pipeline.vae.device).to(pipeline.vae.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af6b09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output.mp4'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.enable_vae_slicing()\n",
    "pipeline.enable_vae_tiling()\n",
    "pipeline.enable_model_cpu_offload()\n",
    "from diffusers.utils import export_to_video\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    has_latents_mean = hasattr(pipeline.vae.config, \"latents_mean\") and pipeline.vae.config.latents_mean is not None\n",
    "    has_latents_std = hasattr(pipeline.vae.config, \"latents_std\") and pipeline.vae.config.latents_std is not None\n",
    "    if has_latents_mean and has_latents_std:\n",
    "        latents_mean = (\n",
    "            torch.tensor(pipeline.vae.config.latents_mean).view(1, 12, 1, 1, 1).to(latents.device, latents.dtype)\n",
    "        )\n",
    "        latents_std = (\n",
    "            torch.tensor(pipeline.vae.config.latents_std).view(1, 12, 1, 1, 1).to(latents.device, latents.dtype)\n",
    "        )\n",
    "        latents_ = latents * latents_std / pipeline.vae.config.scaling_factor + latents_mean\n",
    "    else:\n",
    "        latents_ = latents / pipeline.vae.config.scaling_factor\n",
    "\n",
    "    video = pipeline.vae.decode(latents_, return_dict=False)[0]\n",
    "    video = pipeline.video_processor.postprocess_video(video, output_type=\"pil\")\n",
    "\n",
    "export_to_video(\n",
    "    video[0],\n",
    "    \"output.mp4\",\n",
    "    fps=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aadfff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 'a', '▁cam', 'f', 'la', 'gue', 'd', '▁*', 'anim', 'al', '*', '▁moving', '▁in', '▁the', '▁+', 'back', 'ground', '+']\n",
      "tensor([ 8,  9, 15, 16])\n",
      "tensor([1, 1, 0, 0])\n",
      "tensor([0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "tokenizer = pipeline.tokenizer\n",
    "prompt = \"a camflagued *animal* moving in the +background+\"\n",
    "positive_start_index = tokenizer.tokenize(prompt).index(\"▁*\")\n",
    "positive_end_index = tokenizer.tokenize(prompt).index(\"*\")\n",
    "\n",
    "negative_start_index = tokenizer.tokenize(prompt).index(\"▁+\")\n",
    "negative_end_index = tokenizer.tokenize(prompt).index(\"+\")\n",
    "\n",
    "print(tokenizer.tokenize(prompt))\n",
    "\n",
    "\n",
    "indices = torch.tensor(list(range(positive_start_index + 1, positive_end_index)) + list(range(negative_start_index + 1, negative_end_index)))# + 1 # plus one because of the <extra_id_0> token\n",
    "print(indices)\n",
    "positive_mask = torch.tensor([1] * (positive_end_index - positive_start_index - 1) + [0] * (negative_end_index - negative_start_index - 1))\n",
    "negative_mask = torch.tensor([0] * (positive_end_index - positive_start_index - 1) + [1] * (negative_end_index - negative_start_index - 1))\n",
    "print(positive_mask)\n",
    "print(negative_mask)\n",
    "\n",
    "threshold_noise = 0.025\n",
    "sigmas = linear_quadratic_schedule(20, threshold_noise)\n",
    "sigmas = np.array(sigmas)\n",
    "timesteps = retrieve_timesteps(\n",
    "            pipeline.scheduler,\n",
    "            num_inference_steps=20,\n",
    "            device=\"cuda\",\n",
    "            timesteps=None,\n",
    "            sigmas=sigmas,\n",
    "        )\n",
    "\n",
    "for block in pipeline.transformer.transformer_blocks:\n",
    "    block.attn1.processor = MochiAttnProcessor2_0(token_index_of_interest=indices, positive_mask=positive_mask) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "69d6b3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn_like(latents)\n",
    "# latents = (latents - latents_mean) / latents_std\n",
    "# noisy_latents = pipeline.scheduler.scale_noise(latents, timesteps[0][15:16], noise) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58e35631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline.scheduler.num_inference_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "53ca7ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     has_latents_mean = hasattr(pipeline.vae.config, \"latents_mean\") and pipeline.vae.config.latents_mean is not None\n",
    "#     has_latents_std = hasattr(pipeline.vae.config, \"latents_std\") and pipeline.vae.config.latents_std is not None\n",
    "#     if has_latents_mean and has_latents_std:\n",
    "#         latents_mean = (\n",
    "#             torch.tensor(pipeline.vae.config.latents_mean).view(1, 12, 1, 1, 1).to(latents.device, latents.dtype)\n",
    "#         )\n",
    "#         latents_std = (\n",
    "#             torch.tensor(pipeline.vae.config.latents_std).view(1, 12, 1, 1, 1).to(latents.device, latents.dtype)\n",
    "#         )\n",
    "#         latents_ = noisy_latents * latents_std / pipeline.vae.config.scaling_factor + latents_mean\n",
    "#     else:\n",
    "#         latents_ = noisy_latents / pipeline.vae.config.scaling_factor\n",
    "\n",
    "#     video = pipeline.vae.decode(latents_, return_dict=False)[0]\n",
    "#     video = pipeline.video_processor.postprocess_video(video, output_type=\"pil\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cc68bfbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output.mp4'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_to_video(\n",
    "    video[0],\n",
    "    \"output.mp4\",\n",
    "    fps=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1714d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c3c03b0fad4180a752cd674f3481e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.12/site-packages/diffusers/image_processor.py:147: RuntimeWarning: invalid value encountered in cast\n",
      "  images = (images * 255).round().astype(\"uint8\")\n"
     ]
    }
   ],
   "source": [
    "from diffusers.utils import export_to_video\n",
    "video = pipeline(#latents=noise.cuda(), \n",
    "        #  timesteps=timesteps[0].cpu(),#[:15].cpu(),\n",
    "         num_inference_steps=10,\n",
    "         prompt=prompt, \n",
    "         negative_prompt=\"this is a nagative prompt\",\n",
    "         emphasize_indices=(0,0), \n",
    "         num_frames=12,\n",
    "         emphasize_neg_indices=(0,0),\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f6bd2fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output.mp4'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_to_video(video[\"frames\"][0], \"output.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d802a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = torchvision.io.read_video(str(\"/home/wg25r/make_it_move/training_free/finetrainers/training/mochi-1/dataset/crab_000.mp4\"), output_format=\"THWC\", pts_unit=\"secs\")[0]\n",
    "extracted_positive_maps = [] \n",
    "extracted_negative_maps = []\n",
    "maps = pipeline.attention_maps  \n",
    "for step in range(max(len(maps) - 10, 0), len(maps)-1):\n",
    "    for layer in range(len(maps[step])): \n",
    "        # print(maps[step][layer].shape)# [B, H, K, Q]\n",
    "        map = maps[step][layer][0].mean(0)[0] # use sum, because in it it was softmax and spread outed\n",
    "        # print(maps[step][layer][0].mean(0)[positive_mask==1].shape) # need to use bool!\n",
    "        extracted_positive_maps.append(map.cpu().float().numpy().reshape(-1, frames[0].shape[0]//16, frames[0].shape[1]//16))\n",
    "        \n",
    "        map = maps[step][layer][0].mean(0)[1]\n",
    "        extracted_negative_maps.append(map.cpu().float().numpy().reshape(-1, frames[0].shape[0]//16, frames[0].shape[1]//16))\n",
    "\n",
    "        # assert torch.all(maps[step][layer][0].mean(0).sum(0) >= 0.99), maps[step][layer][0].mean(0).sum(0)\n",
    "extracted_positive_maps = np.array(extracted_positive_maps)\n",
    "extracted_negative_maps = np.array(extracted_negative_maps)\n",
    "np.isnan(extracted_positive_maps).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c136992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "file_id = 1\n",
    "# %%\n",
    "mask = (extracted_positive_maps > extracted_positive_maps.mean(axis=0) + extracted_positive_maps.std(axis=0)).astype(np.float32) + (extracted_positive_maps < extracted_positive_maps.mean(axis=0) - extracted_positive_maps.std(axis=0)).astype(np.float32)\n",
    "mean_pos_map = np.nanmean(extracted_positive_maps, axis=0)\n",
    "\n",
    "\n",
    "mask = (extracted_negative_maps > extracted_negative_maps.mean(axis=0) + extracted_negative_maps.std(axis=0)).astype(np.float32) + (extracted_negative_maps < extracted_negative_maps.mean(axis=0) - extracted_negative_maps.std(axis=0)).astype(np.float32)\n",
    "mean_neg_map = np.nanmean(extracted_negative_maps, axis=0)\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "def opening(x, kernel_size=3):\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))\n",
    "    x = [cv2.morphologyEx(i, cv2.MORPH_OPEN, kernel) for i in x]\n",
    "    return x\n",
    "\n",
    "def blur(x, kernel_size=3):\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.float32) / (kernel_size * kernel_size)\n",
    "    x = [cv2.filter2D(i, -1, kernel) for i in x]\n",
    "    return x\n",
    "\n",
    "def resize(x, size):\n",
    "    return [cv2.resize(i, size) for i in x]\n",
    "    \n",
    "    \n",
    "def normalize(x):\n",
    "    return [(i - i.min()) / (i.max() - i.min()) for i in x]\n",
    "\n",
    "def compose_frames(frames, maps):\n",
    "    frames = [np.array(frame).astype(float)/255 for frame in frames]\n",
    "    for i in range(len(frames)):\n",
    "        map = cv2.cvtColor(maps[(i-2)//6], cv2.COLOR_GRAY2RGB).astype(float)       \n",
    "        frames[i] = cv2.addWeighted(frames[i], 0.5, map, 0.5, 0)\n",
    "    return frames\n",
    "\n",
    "def repeat_maps(maps, total_len):\n",
    "    export_maps = []\n",
    "    for i in range(total_len):\n",
    "        export_maps.append(maps[math.ceil((i-2)/6)])\n",
    "    return export_maps\n",
    "\n",
    "# print(\"a\")\n",
    "mean_pos_map = opening(mean_pos_map, kernel_size=3)\n",
    "mean_neg_map = opening(mean_neg_map, kernel_size=3)\n",
    "mean_pos_map = blur(mean_pos_map, kernel_size=3)\n",
    "mean_neg_map = blur(mean_neg_map, kernel_size=3)\n",
    "mean_pos_map = resize(mean_pos_map, size=(frames[0].shape[1], frames[0].shape[0]))\n",
    "mean_neg_map = resize(mean_neg_map, size=(frames[0].shape[1], frames[0].shape[0]))\n",
    "\n",
    "mean_pos_map = normalize(mean_pos_map)\n",
    "mean_neg_map = normalize(mean_neg_map)\n",
    "\n",
    "video = repeat_maps(mean_pos_map, len(frames))\n",
    "\n",
    "# video = compose_frames(frames, mean_pos_map)\n",
    "print(np.array(video).shape)\n",
    "export_to_video(video, f\"res/mochi_pos_{file_id:02d}_map.mp4\", fps=30)\n",
    "\n",
    "video = repeat_maps(mean_neg_map, len(frames))\n",
    "# video = compose_frames(frames, mean_neg_map)\n",
    "export_to_video(video, f\"res/mochi_neg_{file_id:02d}_map.mp4\", fps=30)\n",
    "\n",
    "# fg = np.clip(np.array(mean_pos_map) - np.array(mean_neg_map), 0, 100)\n",
    "fg = np.array(mean_pos_map) - np.array(mean_neg_map) * 2\n",
    "fg[fg < 0] = 0\n",
    "print(fg.max())\n",
    "\n",
    "fg = normalize(fg)\n",
    "video = compose_frames(frames, fg) \n",
    "export_to_video(video, f\"map.mp4\", fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7853927",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames[0].shape, fg[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
