{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12ca27dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wg25r/miniconda/envs/mochi/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "from diffusers import AutoencoderKLMochi\n",
    "from mochi_pipeline import MochiPipeline, linear_quadratic_schedule, retrieve_timesteps\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "from mochi_processor import MochiAttnProcessor2_0\n",
    "import torch\n",
    "import torchvision\n",
    "from pathlib import Path\n",
    "from diffusers.utils import export_to_video\n",
    "\n",
    "\n",
    "def encode_videos(model: torch.nn.Module, vid_path: Path):\n",
    "    video, _, metadata = torchvision.io.read_video(str(vid_path), output_format=\"THWC\", pts_unit=\"secs\")\n",
    "    video = video.permute(3, 0, 1, 2)\n",
    "    og_shape = video.shape\n",
    "    \n",
    "    print(f\"Original video shape: {og_shape}\")\n",
    "    video = video.unsqueeze(0)\n",
    "    video = video.float() / 127.5 - 1.0\n",
    "    video = video.to(model.device)\n",
    "\n",
    "    assert video.ndim == 5\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            ldist = model._encode(video)\n",
    "    return ldist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cceeb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16a29545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.15it/s]it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:02<00:00,  1.83it/s]it/s]\n",
      "Loading pipeline components...: 100%|██████████| 5/5 [00:04<00:00,  1.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MochiPipeline {\n",
       "  \"_class_name\": \"MochiPipeline\",\n",
       "  \"_diffusers_version\": \"0.34.0.dev0\",\n",
       "  \"_name_or_path\": \"genmo/mochi-1-preview\",\n",
       "  \"force_zeros_for_empty_prompt\": false,\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"FlowMatchEulerDiscreteScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"T5EncoderModel\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"T5Tokenizer\"\n",
       "  ],\n",
       "  \"transformer\": [\n",
       "    \"diffusers\",\n",
       "    \"MochiTransformer3DModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKLMochi\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = \"genmo/mochi-1-preview\"\n",
    "pipeline = MochiPipeline.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n",
    "\n",
    "pipeline.enable_vae_slicing()\n",
    "pipeline.enable_vae_tiling()\n",
    "pipeline.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17e357c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wg25r/miniconda/envs/mochi/lib/python3.11/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n",
      "/home/wg25r/miniconda/envs/mochi/lib/python3.11/site-packages/torchvision/io/video.py:199: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original video shape: torch.Size([3, 81, 480, 848])\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    video = \"./cog_dataset/videos/crab_1_001.mp4\"\n",
    "    latents = encode_videos(\n",
    "        pipeline.vae,\n",
    "        Path(video),\n",
    "    )[:,:12].to(pipeline.vae.device).to(pipeline.vae.dtype) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2af6b09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output.mp4'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.enable_vae_slicing()\n",
    "pipeline.enable_vae_tiling()\n",
    "pipeline.enable_model_cpu_offload()\n",
    "from diffusers.utils import export_to_video\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    has_latents_mean = hasattr(pipeline.vae.config, \"latents_mean\") and pipeline.vae.config.latents_mean is not None\n",
    "    has_latents_std = hasattr(pipeline.vae.config, \"latents_std\") and pipeline.vae.config.latents_std is not None\n",
    "    if has_latents_mean and has_latents_std:\n",
    "        latents_mean = (\n",
    "            torch.tensor(pipeline.vae.config.latents_mean).view(1, 12, 1, 1, 1).to(latents.device, latents.dtype)\n",
    "        )\n",
    "        latents_std = (\n",
    "            torch.tensor(pipeline.vae.config.latents_std).view(1, 12, 1, 1, 1).to(latents.device, latents.dtype)\n",
    "        )\n",
    "        latents_ = latents * latents_std / pipeline.vae.config.scaling_factor + latents_mean\n",
    "    else:\n",
    "        latents_ = latents / pipeline.vae.config.scaling_factor\n",
    "\n",
    "    video = pipeline.vae.decode(latents_, return_dict=False)[0]\n",
    "    video = pipeline.video_processor.postprocess_video(video, output_type=\"pil\")\n",
    "\n",
    "export_to_video(\n",
    "    video[0],\n",
    "    \"output.mp4\",\n",
    "    fps=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3aadfff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 'a', '▁cam', 'f', 'la', 'gue', 'd', '▁*', 'anim', 'al', '*', '▁moving', '▁in', '▁the', '▁+', 'back', 'ground', '+']\n",
      "tensor([ 8,  9, 15, 16])\n",
      "tensor([1, 1, 0, 0])\n",
      "tensor([0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "tokenizer = pipeline.tokenizer\n",
    "prompt = \"a camflagued *animal* moving in the +background+\"\n",
    "positive_start_index = tokenizer.tokenize(prompt).index(\"▁*\")\n",
    "positive_end_index = tokenizer.tokenize(prompt).index(\"*\")\n",
    "\n",
    "negative_start_index = tokenizer.tokenize(prompt).index(\"▁+\")\n",
    "negative_end_index = tokenizer.tokenize(prompt).index(\"+\")\n",
    "\n",
    "print(tokenizer.tokenize(prompt))\n",
    "\n",
    "\n",
    "indices = torch.tensor(list(range(positive_start_index + 1, positive_end_index)) + list(range(negative_start_index + 1, negative_end_index)))# + 1 # plus one because of the <extra_id_0> token\n",
    "print(indices)\n",
    "positive_mask = torch.tensor([1] * (positive_end_index - positive_start_index - 1) + [0] * (negative_end_index - negative_start_index - 1))\n",
    "negative_mask = torch.tensor([0] * (positive_end_index - positive_start_index - 1) + [1] * (negative_end_index - negative_start_index - 1))\n",
    "print(positive_mask)\n",
    "print(negative_mask)\n",
    "\n",
    "threshold_noise = 0.025\n",
    "sigmas = linear_quadratic_schedule(20, threshold_noise)\n",
    "sigmas = np.array(sigmas)\n",
    "timesteps = retrieve_timesteps(\n",
    "            pipeline.scheduler,\n",
    "            num_inference_steps=20,\n",
    "            device=\"cuda\",\n",
    "            timesteps=None,\n",
    "            sigmas=sigmas,\n",
    "        )\n",
    "\n",
    "for block in pipeline.transformer.transformer_blocks:\n",
    "    block.attn1.processor = MochiAttnProcessor2_0(token_index_of_interest=indices, positive_mask=positive_mask) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69d6b3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn_like(latents)\n",
    "latents = (latents - latents_mean) / latents_std \n",
    "noisy_latents = pipeline.scheduler.scale_noise(latents, timesteps[0][15:16], noise) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58e35631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline.scheduler.num_inference_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53ca7ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     has_latents_mean = hasattr(pipeline.vae.config, \"latents_mean\") and pipeline.vae.config.latents_mean is not None\n",
    "#     has_latents_std = hasattr(pipeline.vae.config, \"latents_std\") and pipeline.vae.config.latents_std is not None\n",
    "#     if has_latents_mean and has_latents_std:\n",
    "#         latents_mean = (\n",
    "#             torch.tensor(pipeline.vae.config.latents_mean).view(1, 12, 1, 1, 1).to(latents.device, latents.dtype)\n",
    "#         )\n",
    "#         latents_std = (\n",
    "#             torch.tensor(pipeline.vae.config.latents_std).view(1, 12, 1, 1, 1).to(latents.device, latents.dtype)\n",
    "#         )\n",
    "#         latents_ = noisy_latents * latents_std / pipeline.vae.config.scaling_factor + latents_mean\n",
    "#     else:\n",
    "#         latents_ = noisy_latents / pipeline.vae.config.scaling_factor\n",
    "\n",
    "#     video = pipeline.vae.decode(latents_, return_dict=False)[0]\n",
    "#     video = pipeline.video_processor.postprocess_video(video, output_type=\"pil\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc68bfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export_to_video(\n",
    "#     video[0],\n",
    "#     \"output.mp4\",\n",
    "#     fps=30\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb1714d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [01:25<04:16, 17.11s/it]\n"
     ]
    }
   ],
   "source": [
    "from diffusers.utils import export_to_video\n",
    "video = pipeline(latents=noisy_latents.cuda(), \n",
    "         num_inference_steps=20,\n",
    "         start_index=15,\n",
    "         prompt=prompt, \n",
    "         negative_prompt=\"this is a nagative prompt\",\n",
    "         emphasize_indices=(0,0), \n",
    "         emphasize_neg_indices=(0,0),\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6bd2fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output.mp4'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_to_video(video[\"frames\"][0], \"output.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16d802a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.False_"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = torchvision.io.read_video(str(\"./cog_dataset/videos/crab_1_001.mp4\"), output_format=\"THWC\", pts_unit=\"secs\")[0]\n",
    "extracted_positive_maps = [] \n",
    "extracted_negative_maps = []\n",
    "maps = pipeline.attention_maps  \n",
    "for step in range(max(len(maps) - 10, 0), len(maps)-1):\n",
    "    for layer in range(len(maps[step])): \n",
    "        # print(maps[step][layer].shape)# [B, H, K, Q]\n",
    "        map = maps[step][layer][0].mean(0)[0] # use sum, because in it it was softmax and spread outed\n",
    "        # print(maps[step][layer][0].mean(0)[positive_mask==1].shape) # need to use bool!\n",
    "        extracted_positive_maps.append(map.cpu().float().numpy().reshape(-1, frames[0].shape[0]//16, frames[0].shape[1]//16))\n",
    "        \n",
    "        map = maps[step][layer][0].mean(0)[1]\n",
    "        extracted_negative_maps.append(map.cpu().float().numpy().reshape(-1, frames[0].shape[0]//16, frames[0].shape[1]//16))\n",
    "\n",
    "        # assert torch.all(maps[step][layer][0].mean(0).sum(0) >= 0.99), maps[step][layer][0].mean(0).sum(0)\n",
    "extracted_positive_maps = np.array(extracted_positive_maps)\n",
    "extracted_negative_maps = np.array(extracted_negative_maps)\n",
    "np.isnan(extracted_positive_maps).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c136992e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81, 480, 848)\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_923480/758690003.py:33: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  frames = [np.array(frame).astype(float)/255 for frame in frames]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'map.mp4'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "file_id = 1\n",
    "# %%\n",
    "mask = (extracted_positive_maps > extracted_positive_maps.mean(axis=0) + extracted_positive_maps.std(axis=0)).astype(np.float32) + (extracted_positive_maps < extracted_positive_maps.mean(axis=0) - extracted_positive_maps.std(axis=0)).astype(np.float32)\n",
    "mean_pos_map = np.nanmean(extracted_positive_maps, axis=0)\n",
    "\n",
    "\n",
    "mask = (extracted_negative_maps > extracted_negative_maps.mean(axis=0) + extracted_negative_maps.std(axis=0)).astype(np.float32) + (extracted_negative_maps < extracted_negative_maps.mean(axis=0) - extracted_negative_maps.std(axis=0)).astype(np.float32)\n",
    "mean_neg_map = np.nanmean(extracted_negative_maps, axis=0)\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "def opening(x, kernel_size=3):\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))\n",
    "    x = [cv2.morphologyEx(i, cv2.MORPH_OPEN, kernel) for i in x]\n",
    "    return x\n",
    "\n",
    "def blur(x, kernel_size=3):\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.float32) / (kernel_size * kernel_size)\n",
    "    x = [cv2.filter2D(i, -1, kernel) for i in x]\n",
    "    return x\n",
    "\n",
    "def resize(x, size):\n",
    "    return [cv2.resize(i, size) for i in x]\n",
    "    \n",
    "    \n",
    "def normalize(x):\n",
    "    return [(i - i.min()) / (i.max() - i.min()) for i in x]\n",
    "\n",
    "def compose_frames(frames, maps):\n",
    "    frames = [np.array(frame).astype(float)/255 for frame in frames]\n",
    "    for i in range(len(frames)):\n",
    "        map = cv2.cvtColor(maps[(i-2)//6], cv2.COLOR_GRAY2RGB).astype(float)       \n",
    "        frames[i] = cv2.addWeighted(frames[i], 0.5, map, 0.5, 0)\n",
    "    return frames\n",
    "\n",
    "def repeat_maps(maps, total_len):\n",
    "    export_maps = []\n",
    "    for i in range(total_len):\n",
    "        export_maps.append(maps[math.ceil((i-2)/6)])\n",
    "    return export_maps\n",
    "\n",
    "# print(\"a\")\n",
    "mean_pos_map = opening(mean_pos_map, kernel_size=3)\n",
    "mean_neg_map = opening(mean_neg_map, kernel_size=3)\n",
    "mean_pos_map = blur(mean_pos_map, kernel_size=3)\n",
    "mean_neg_map = blur(mean_neg_map, kernel_size=3)\n",
    "mean_pos_map = resize(mean_pos_map, size=(frames[0].shape[1], frames[0].shape[0]))\n",
    "mean_neg_map = resize(mean_neg_map, size=(frames[0].shape[1], frames[0].shape[0]))\n",
    "\n",
    "mean_pos_map = normalize(mean_pos_map)\n",
    "mean_neg_map = normalize(mean_neg_map)\n",
    "\n",
    "video = repeat_maps(mean_pos_map, len(frames))\n",
    "\n",
    "# video = compose_frames(frames, mean_pos_map)\n",
    "print(np.array(video).shape)\n",
    "export_to_video(video, f\"res/mochi_pos_{file_id:02d}_map.mp4\", fps=30)\n",
    "\n",
    "video = repeat_maps(mean_neg_map, len(frames))\n",
    "# video = compose_frames(frames, mean_neg_map)\n",
    "export_to_video(video, f\"res/mochi_neg_{file_id:02d}_map.mp4\", fps=30)\n",
    "\n",
    "# fg = np.clip(np.array(mean_pos_map) - np.array(mean_neg_map), 0, 100)\n",
    "fg = np.array(mean_pos_map) - np.array(mean_neg_map) * 2\n",
    "fg[fg < 0] = 0\n",
    "print(fg.max())\n",
    "\n",
    "fg = normalize(fg)\n",
    "video = compose_frames(frames, fg) \n",
    "export_to_video(video, f\"map.mp4\", fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7853927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([480, 848, 3]), (480, 848))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames[0].shape, fg[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mochi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
